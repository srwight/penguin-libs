{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.5-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37564bitbasecondab9032d6a004743e5afb3f7b66a8d4b8a",
   "display_name": "Python 3.7.5 64-bit ('base': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machine\n",
    "#### Linear Regression using scikit-learn's support vector machine\n",
    "Stephen Wight - author"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Project Aims\n",
    "\n",
    "This is one portion of the Yelp Reviews project being undertaken by George Avitesyan, Alex Buckalew, Desmong Henderson, Michael Sriqui, and Stephen Wight\n",
    "\n",
    "The aim is to create multiple algorithms using different ML concepts, then merge them in an ensemble-learning process. This notebook represents one ML algorithm - the Support Vector Machine and its Linear Regression implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Importing Libraries\n",
    "##### Scikit-Learn\n",
    "This project leans heavily on the Scikit-Learn libraries for machine learning. It implements:\n",
    "* Support Vector Machine\n",
    "    * Linear Regression\n",
    "* Feature Extraction\n",
    "    * Text\n",
    "        * TF/IDF Vectorizer\n",
    "* Model Selection\n",
    "    * Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Contractions, json, and string.punctuation\n",
    "I'm also using a module, 'contractions', to expand any contractions that are in the text.\n",
    "\n",
    "The data is stored in JavaScript Object Notation (JSON), so to access it, I need to use a library that interprets the JSON objects and returns them as strings.\n",
    "\n",
    "Next, I'm importing a constant which includes the most commonly used punctuation, so that I can strip punctuation marks from the text.\n",
    "\n",
    "Finally, I'm using the pickle library, which allows me to save a python object as a binary file that I can import later. This is useful for moving my trained model from this demonstration into a predicition algorithm. For this purpose, I only need the \"dumps\" function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions\n",
    "from json import loads\n",
    "from string import punctuation\n",
    "from pickle import dumps"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Custom Functions\n",
    "#### demotify()\n",
    "To preprocess my data, I needed a few custom functions. The first is \"demotify\", which replaces common emoticon strings like :) and :( with words like 'emhappy' and 'emsad', so that they don't get stripped away with the punctuation in a later step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demotify(indoc:str) -> str:\n",
    "    \"\"\"demotify - remove select emoticons from strings, then return the strings\n",
    "\n",
    "    This function removes any emoticons (the precursor to graphical emoji) from \n",
    "    a string of text, replacing them with a word that represents their emotional\n",
    "    content.\n",
    "\n",
    "    Args:\n",
    "        indoc: This is the string that may contain emoticons to be replaced.\n",
    "\n",
    "    Returns:\n",
    "        str: This function returns the string with emoticons replaced by words\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    emoticon_dict = {\n",
    "        ':)': ' emhappy ',\n",
    "        ':-)':' emhappy ',\n",
    "        ':(': ' emsad ',\n",
    "        ':-(':' emsad ',\n",
    "        '>:(':' emangry ',\n",
    "        ':D': ' emgrin ',\n",
    "        ':-D':' emgrin ',\n",
    "        ';)': ' emwink ',\n",
    "        ';-)':' emwink '\n",
    "    }\n",
    "\n",
    "    for key in emoticon_dict:\n",
    "        indoc = indoc.replace(key, emoticon_dict[key])\n",
    "\n",
    "    return indoc"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### getsample()\n",
    "\n",
    "Getsample is a pretty important function. This is where I collect single reviews and their related star ratings and preprocess them. I process them in the following order:\n",
    "* demotify() - replace emoticons\n",
    "* convert to lowercase\n",
    "* contractions.fix() - expand contractions\n",
    "* str.translate - remove punctuation\n",
    "\n",
    "In addition, this function is set up to select a varied sample-size from varied starting points in the data.\n",
    "\n",
    "This function is also where I would normally check for language using a library called langdetect, but Alex has already preproccessed the entire dataset and filtered only those which detect as being in English. That section has been commented out for this demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getsample(infile, numsamples, startpoint=0):\n",
    "\n",
    "    ### Declare the variable outside of the loops\n",
    "    outlist = []\n",
    "    with open(infile, 'rt') as fl_in:\n",
    "\n",
    "        ## Enumerate is nice here, because it does not load the entire file into memory, but rather loads a line at a time.\n",
    "        for i,x in enumerate(fl_in):\n",
    "            ## Keep moving until you get to the start point\n",
    "            if i < startpoint:\n",
    "                continue\n",
    "\n",
    "            ## When the enumerate function reaches the desired start point, let the user know\n",
    "            if i == startpoint:\n",
    "                print(f'Beginning at {i}')\n",
    "\n",
    "            ## It's useful with long processes to have an output that tells the user that things are still working!\n",
    "            ## This line prints a status every 10000 lines.\n",
    "            if len(outlist) % 10000 == 0: print(f'\\rRetrieved {len(outlist)}th record.', end='')\n",
    "\n",
    "            ## Select only the portions of the line that we need\n",
    "            x = dict(loads(x))\n",
    "\n",
    "            ## Remove emoticons\n",
    "            x['text'] = demotify(x['text'])\n",
    "\n",
    "            ## Check for english language\n",
    "            # try:\n",
    "            #     if detect(x['text']) != 'en':\n",
    "            #         continue\n",
    "            # except:\n",
    "            #     continue\n",
    "\n",
    "            ## convert to lowercase\n",
    "            x['text'] = x['text'].lower()\n",
    "\n",
    "            ## expand contractions\n",
    "            x['text'] = contractions.fix(x['text'])\n",
    "\n",
    "            ## remove punctuation\n",
    "            x['text'] = x['text'].translate(str.maketrans('','',punctuation))\n",
    "\n",
    "            ## Add the current line to the list to be returned\n",
    "            outlist.append(x)\n",
    "\n",
    "            ## Check to see if we have enough samples. If we do, break the loop. If not, let it continue.\n",
    "            if len(outlist) >= numsamples:\n",
    "                print(f'\\n{len(outlist)} samples retrieved.')\n",
    "                break\n",
    "\n",
    "    ## Return our completed list.    \n",
    "    return outlist"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### The Training Function\n",
    "This function encapsulates the entire training process. I call this function with an untrained model, a filename, a sample size, and a start location (so it can pass them to the getsample function), and it returns a trained model and an accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model:LinearSVR, filein:str, samplesize:int, startloc:int=0, tts=0.8) -> tuple:\n",
    "\n",
    "    ## get a sample from the dataset \n",
    "    data = getsample(filein, samplesize, startloc)\n",
    "\n",
    "    ## Split off the test variable\n",
    "    text = []\n",
    "    stars = []\n",
    "    for record in data:\n",
    "        text.append(record['text'])\n",
    "        stars.append(record['stars'])\n",
    "\n",
    "    ## Initialize the TF/IDF vectorizer\n",
    "    vec = TfidfVectorizer()\n",
    "\n",
    "    ## Fit and transform the vectorizer \n",
    "    vectors = vec.fit_transform(text)\n",
    "\n",
    "    ## Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(vectors,stars,test_size=tts)\n",
    "\n",
    "    model.fit(X_train,y_train)\n",
    "    r_squared = model.score(X_test, y_test)\n",
    "\n",
    "    return model, r_squared"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Benchmarks\n",
    "Now that all of the functions are laid out, let's approach training and tuning our model's hyperparamenters.\n",
    "\n",
    "I want to begin with a very small dataset, just to be sure everything is working.\n",
    "#### Sample Size - 1000\n",
    "I'll start with 1000 samples right off the top, and I'll use all of the default settings on the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Beginning at 0\nRetrieved 0th record.\n1000 samples retrieved.\nThis model represents 17.93% of the sentiment in the text.\n"
    }
   ],
   "source": [
    "## Everything here is the default settings.\n",
    "model = LinearSVR(\n",
    "    epsilon=0.0,\n",
    "    tol = 1e-4,\n",
    "    C=1.0,\n",
    "    loss='epsilon_insensitive',\n",
    "    fit_intercept=True,\n",
    "    intercept_scaling=1.0,\n",
    "    dual=True,\n",
    "    verbose=0,\n",
    "    random_state=None,\n",
    "    max_iter=1000\n",
    ")\n",
    "\n",
    "svr_default_1000, score = training(\n",
    "    model = model, \n",
    "    filein = '/home/srwight/Documents/Revature/Group Project/Yelp/nlp/english_only_reviews.json', \n",
    "    samplesize = 1000\n",
    ")\n",
    "\n",
    "print(f'This model represents {round(score*100,2)}% of the sentiment in the text.')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Sample Size 1,000 - result\n",
    "As one would expect, with only 1,000 samples, the model did not perform very well. But that's to be expected. A 1000 piece sample is really just to be sure everything's running smoothly.\n",
    "\n",
    "#### Sample Size - 10,000\n",
    "This will likely have a similar result - but the hope is that the accuracy improves significantly, even if it doesn't reach production-level standards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Beginning at 0\nRetrieved 0th record.\n10000 samples retrieved.\nThis model represents 54.14% of the sentiment in the text.\n"
    }
   ],
   "source": [
    "## Everything here is the default settings.\n",
    "model = LinearSVR(\n",
    "    epsilon=0.0,\n",
    "    tol = 1e-4,\n",
    "    C=1.0,\n",
    "    loss='epsilon_insensitive',\n",
    "    fit_intercept=True,\n",
    "    intercept_scaling=1.0,\n",
    "    dual=True,\n",
    "    verbose=0,\n",
    "    random_state=None,\n",
    "    max_iter=1000\n",
    ")\n",
    "\n",
    "svr_default_10000, score = training(\n",
    "    model = model, \n",
    "    filein = '/home/srwight/Documents/Revature/Group Project/Yelp/nlp/english_only_reviews.json', \n",
    "    samplesize = 10000\n",
    ")\n",
    "\n",
    "print(f'This model represents {round(score*100,2)}% of the sentiment in the text.')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Sample Size 10,000 Results\n",
    "The accuracy of the model jumped by a factor of 3. It is now representing twice as much of the sentiment in the text as it did with only 1000 samples.\n",
    "\n",
    "## BENCHMARK\n",
    "#### Sample Size 100,000\n",
    "\n",
    "The benchmark for our team is at the 100,000 sample mark. After running at 100,000 samples, we can begin adjusting the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Beginning at 0\nRetrieved 90000th record.\n100000 samples retrieved.\nThis model represents 63.47% of the sentiment in the text.\n"
    }
   ],
   "source": [
    "## Everything here is the default settings.\n",
    "model = LinearSVR(\n",
    "    epsilon=0.0,\n",
    "    tol = 1e-4,\n",
    "    C=1.0,\n",
    "    loss='epsilon_insensitive',\n",
    "    fit_intercept=True,\n",
    "    intercept_scaling=1.0,\n",
    "    dual=True,\n",
    "    verbose=0,\n",
    "    random_state=None,\n",
    "    max_iter=1000\n",
    ")\n",
    "\n",
    "svr_default_100000, score = training(\n",
    "    model = model, \n",
    "    filein = '/home/srwight/Documents/Revature/Group Project/Yelp/nlp/english_only_reviews.json', \n",
    "    samplesize = 100000\n",
    ")\n",
    "\n",
    "print(f'This model represents {round(score*100,2)}% of the sentiment in the text.')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BENCHMARK RESULTS\n",
    "#### 100,000 sample results\n",
    "\n",
    "With 100,000 samples, the default settings on the LinearSVR model result in a 63.47% R-squared rating. Given that this is roughly 3% of the available data, I feel that this is a good start. "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyperparameters\n",
    "\n",
    "Hyperparameters are the small adjustments one can make when declaring a Machine Learning model.\n",
    "\n",
    "### Tangential Parameters\n",
    "The LinearSVR model has ten hyperparameters, but only some can be adjusted. Here are a few that should remain as they are:\n",
    "\n",
    "#### fit_intercept (boolean) - default True\n",
    "*fit_intercept* is simply telling the algorithm whether the data has already been centered. I have made no effort to center the data, so this needs to stay **True**.\n",
    "#### verbose (boolean) - default False\n",
    "This variable does not affect the performance of the model - it only determines whether the model prints status updates as it works. While those might be interesting, they won't affect performance. When I get closer to production-level, I may turn this value on so that I can see a little more about what's going on inside.\n",
    "#### random_state (integer) - default None\n",
    "The only reason to set this to a specific number is to force the 'random' variables to be exactly the same every time. This can be useful when you're comparing runs. For later tests, I will set *random_state* to a specific value so I can isolate other changes.\n",
    "\n",
    "### Operational Parameters\n",
    "Of the remaining seven, a few can be adjusted independently of the others. They include:\n",
    "#### tol - float, default 0.0001\n",
    "*tol* represents tolerance for stopping. This represents how close to the best possible fit we will require the algorithm to get. making this number smaller should increase accuracy, but it will also increase processing time.\n",
    "#### C - float, default 1\n",
    "*C* represents how much we're willing to let the best fit line curve to incorporate the data. Increasing this value will increase accuracy, but it risks overfitting. If overfitting is already a problem, lowering C might combat it. This will become very useful as our dataset size increases.\n",
    "#### dual - boolean, default **True**.\n",
    "*dual* selects whether the algorithm solves the 'dual' or 'primal' optimization problem. Basically, with *dual* set to true, the algorithm is regressing from both sides, while with 'primal', it is only regressing from one. The recommendation is that *dual* be set to **False** when there are more samples than there are features. I believe there are around 100,000 words represented in the text, so with 100,000 samples, setting this to **True** is reasonable. However, as I increase the sample size, setting this to **False** could be useful. Of note, when set to **False**, the algorithm does not use any random values, so random_state is ignored.\n",
    "#### max_iter - integer, default 1000\n",
    "*max_iter* is simply a maximum number of times the algorithm can attempt to regress before it takes what it has as the best it can do. Increasing this value might increase accuracy, but it will also increase processing time.\n",
    "\n",
    "Two of the parameters are linked: **epsilon** and **loss**\n",
    "#### loss - string, default 'epsilon_insensitive'\n",
    "This specifies which loss function to use - L1 or L2. \n",
    "\n",
    "A support vector machine depends on a margin. As we attempt to minimize the cost function, points farther away from that margin cause a penalty to the cost function, and that information informs later iterations of the algorithm.\n",
    "\n",
    "L1 is as follows:\n",
    "\n",
    "$\\frac{1}{2}||w||^2 + C\\sum_{i=1}^{M}\\epsilon$\n",
    "\n",
    "L2 is as follows:\n",
    "\n",
    "$\\frac{1}{2}||w||^2 + \\frac{C}{2}\\sum_{i=1}^{M}\\epsilon^2$\n",
    "\n",
    "L2 imposes a stronger penalty, so it may be more effective, but it may also risk overfitting. When epsilon is zero, this setting doesn't matter.\n",
    "\n",
    "#### epsilon - float, default 0.0\n",
    "This represents the margin used in the cost function above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Experiments with Hyperparameters\n",
    "\n",
    "### tol\n",
    "\n",
    "In this iteration of our project, we decrease the tolerance of the model by a factor of ten. My hypothesis is that this will have some effect, but it will also require increasing max_iter to achieve full effect. I haveverbose** = 1, so that we can weee what kind of output it gives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Beginning at 0\nRetrieved 90000th record.\n100000 samples retrieved.\n[LibLinear]This model represents 63.28% of the sentiment in the text.\n"
    }
   ],
   "source": [
    "model = LinearSVR(\n",
    "    epsilon=0.0,\n",
    "    tol = 1e-5,\n",
    "    C=1.0,\n",
    "    loss='epsilon_insensitive',\n",
    "    fit_intercept=True,\n",
    "    intercept_scaling=1.0,\n",
    "    dual=True,\n",
    "    verbose=1,\n",
    "    random_state=None,\n",
    "    max_iter=1000\n",
    ")\n",
    "\n",
    "svr_tol_e5_100000, score = training(\n",
    "    model = model, \n",
    "    filein = '/home/srwight/Documents/Revature/Group Project/Yelp/nlp/english_only_reviews.json', \n",
    "    samplesize = 100000\n",
    ")\n",
    "\n",
    "print(f'This model represents {round(score*100,2)}% of the sentiment in the text.')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The results are statistically similar to the benchmark results. Also, verbose(1) does not tell us much.\n",
    "\n",
    "I have set the tolerance another factor of ten smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Beginning at 0\nRetrieved 90000th record.\n100000 samples retrieved.\nThis model represents 63.32% of the sentiment in the text.\n"
    }
   ],
   "source": [
    "model = LinearSVR(\n",
    "    epsilon=0.0,\n",
    "    tol = 1e-6,\n",
    "    C=1.0,\n",
    "    loss='epsilon_insensitive',\n",
    "    fit_intercept=True,\n",
    "    intercept_scaling=1.0,\n",
    "    dual=True,\n",
    "    verbose=0,\n",
    "    random_state=None,\n",
    "    max_iter=1000\n",
    ")\n",
    "\n",
    "svr_tol_e6_100000, score = training(\n",
    "    model = model, \n",
    "    filein = '/home/srwight/Documents/Revature/Group Project/Yelp/nlp/english_only_reviews.json', \n",
    "    samplesize = 100000\n",
    ")\n",
    "\n",
    "print(f'This model represents {round(score*100,2)}% of the sentiment in the text.')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "There was still no real change. Perhaps an increase in max_iter would help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Beginning at 0\nRetrieved 90000th record.\n100000 samples retrieved.\nThis model represents 63.33% of the sentiment in the text.\n"
    }
   ],
   "source": [
    "model = LinearSVR(\n",
    "    epsilon=0.0,\n",
    "    tol = 1e-6,\n",
    "    C=1.0,\n",
    "    loss='epsilon_insensitive',\n",
    "    fit_intercept=True,\n",
    "    intercept_scaling=1.0,\n",
    "    dual=True,\n",
    "    verbose=0,\n",
    "    random_state=None,\n",
    "    max_iter=10000\n",
    ")\n",
    "\n",
    "svr_tol_e6_iter_e4_100000, score = training(\n",
    "    model = model, \n",
    "    filein = '/home/srwight/Documents/Revature/Group Project/Yelp/nlp/english_only_reviews.json', \n",
    "    samplesize = 100000\n",
    ")\n",
    "\n",
    "print(f'This model represents {round(score*100,2)}% of the sentiment in the text.')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The improvements are minimal. In the next experiments, these will return to default.\n",
    "\n",
    "### C\n",
    "\n",
    "This parameter has the potential to quickly overfit our model and significantly increase processing time. I will proceed with caution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Beginning at 0\nRetrieved 90000th record.\n100000 samples retrieved.\nThis model represents 61.28% of the sentiment in the text.\n"
    }
   ],
   "source": [
    "model = LinearSVR(\n",
    "    epsilon=0.0,\n",
    "    tol = 1e-4,\n",
    "    C=2,\n",
    "    loss='epsilon_insensitive',\n",
    "    fit_intercept=True,\n",
    "    intercept_scaling=1.0,\n",
    "    dual=True,\n",
    "    verbose=0,\n",
    "    random_state=None,\n",
    "    max_iter=1000\n",
    ")\n",
    "\n",
    "svr_C2_100000, score = training(\n",
    "    model = model, \n",
    "    filein = '/home/srwight/Documents/Revature/Group Project/Yelp/nlp/english_only_reviews.json', \n",
    "    samplesize = 100000\n",
    ")\n",
    "\n",
    "print(f'This model represents {round(score*100,2)}% of the sentiment in the text.')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Increasing C actually decreased the accuracy of the model. I suspect overfitting. Perhaps the other direction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Beginning at 0\nRetrieved 90000th record.\n100000 samples retrieved.\nThis model represents 63.93% of the sentiment in the text.\n"
    }
   ],
   "source": [
    "model = LinearSVR(\n",
    "    epsilon=0.0,\n",
    "    tol = 1e-4,\n",
    "    C=0.5,\n",
    "    loss='epsilon_insensitive',\n",
    "    fit_intercept=True,\n",
    "    intercept_scaling=1.0,\n",
    "    dual=True,\n",
    "    verbose=0,\n",
    "    random_state=None,\n",
    "    max_iter=1000\n",
    ")\n",
    "\n",
    "svr_C_0_5_100000, score = training(\n",
    "    model = model, \n",
    "    filein = '/home/srwight/Documents/Revature/Group Project/Yelp/nlp/english_only_reviews.json', \n",
    "    samplesize = 100000\n",
    ")\n",
    "\n",
    "print(f'This model represents {round(score*100,2)}% of the sentiment in the text.')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reducing C to 0.5 represented a moderate increase in accuracy, but still fairly small. Still, it is worth pursuing.\n",
    "\n",
    "C=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Beginning at 0\nRetrieved 90000th record.\n100000 samples retrieved.\nThis model represents 57.82% of the sentiment in the text.\n"
    }
   ],
   "source": [
    "model = LinearSVR(\n",
    "    epsilon=0.0,\n",
    "    tol = 1e-4,\n",
    "    C=0.1,\n",
    "    loss='epsilon_insensitive',\n",
    "    fit_intercept=True,\n",
    "    intercept_scaling=1.0,\n",
    "    dual=True,\n",
    "    verbose=0,\n",
    "    random_state=None,\n",
    "    max_iter=1000\n",
    ")\n",
    "\n",
    "svr_C_0_1_100000, score = training(\n",
    "    model = model, \n",
    "    filein = '/home/srwight/Documents/Revature/Group Project/Yelp/nlp/english_only_reviews.json', \n",
    "    samplesize = 100000\n",
    ")\n",
    "\n",
    "print(f'This model represents {round(score*100,2)}% of the sentiment in the text.')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C of 0.1 decreased accuracy by a large amount. Split the difference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Beginning at 0\nRetrieved 90000th record.\n100000 samples retrieved.\nThis model represents 63.61% of the sentiment in the text.\n"
    }
   ],
   "source": [
    "model = LinearSVR(\n",
    "    epsilon=0.0,\n",
    "    tol = 1e-4,\n",
    "    C=0.3,\n",
    "    loss='epsilon_insensitive',\n",
    "    fit_intercept=True,\n",
    "    intercept_scaling=1.0,\n",
    "    dual=True,\n",
    "    verbose=0,\n",
    "    random_state=None,\n",
    "    max_iter=1000\n",
    ")\n",
    "\n",
    "svr_C_0_5_100000, score = training(\n",
    "    model = model, \n",
    "    filein = '/home/srwight/Documents/Revature/Group Project/Yelp/nlp/english_only_reviews.json', \n",
    "    samplesize = 100000\n",
    ")\n",
    "\n",
    "print(f'This model represents {round(score*100,2)}% of the sentiment in the text.')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "This seems to be a little low still. Let's try 0.8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Beginning at 0\nRetrieved 90000th record.\n100000 samples retrieved.\nThis model represents 63.46% of the sentiment in the text.\n"
    }
   ],
   "source": [
    "model = LinearSVR(\n",
    "    epsilon=0.0,\n",
    "    tol = 1e-4,\n",
    "    C=0.8,\n",
    "    loss='epsilon_insensitive',\n",
    "    fit_intercept=True,\n",
    "    intercept_scaling=1.0,\n",
    "    dual=True,\n",
    "    verbose=0,\n",
    "    random_state=None,\n",
    "    max_iter=1000\n",
    ")\n",
    "\n",
    "svr_C_0_5_100000, score = training(\n",
    "    model = model, \n",
    "    filein = '/home/srwight/Documents/Revature/Group Project/Yelp/nlp/english_only_reviews.json', \n",
    "    samplesize = 100000\n",
    ")\n",
    "\n",
    "print(f'This model represents {round(score*100,2)}% of the sentiment in the text.')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "It seems the default setting is best for this parameter as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Beginning at 0\nRetrieved 90000th record.\n100000 samples retrieved.\nThis model represents 63.93% of the sentiment in the text.\n"
    }
   ],
   "source": [
    "model = LinearSVR(\n",
    "    epsilon=0.0,\n",
    "    tol = 1e-4,\n",
    "    C=1.0,\n",
    "    loss='squared_epsilon_insensitive',\n",
    "    fit_intercept=True,\n",
    "    intercept_scaling=1.0,\n",
    "    dual=False,\n",
    "    verbose=0,\n",
    "    random_state=None,\n",
    "    max_iter=1000\n",
    ")\n",
    "\n",
    "svr_dual_false_E2i_100000, score = training(\n",
    "    model = model, \n",
    "    filein = '/home/srwight/Documents/Revature/Group Project/Yelp/nlp/english_only_reviews.json', \n",
    "    samplesize = 100000\n",
    ")\n",
    "\n",
    "print(f'This model represents {round(score*100,2)}% of the sentiment in the text.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Beginning at 0\nRetrieved 90000th record.\n100000 samples retrieved.\nThis model represents 63.34% of the sentiment in the text.\n"
    }
   ],
   "source": [
    "model = LinearSVR(\n",
    "    epsilon=0.0,\n",
    "    tol = 1e-4,\n",
    "    C=0.8,\n",
    "    loss='epsilon_insensitive',\n",
    "    fit_intercept=True,\n",
    "    intercept_scaling=1.0,\n",
    "    dual=True,\n",
    "    verbose=0,\n",
    "    random_state=None,\n",
    "    max_iter=1000\n",
    ")\n",
    "\n",
    "svr_C_0_5_100000, score = training(\n",
    "    model = model, \n",
    "    filein = '/home/srwight/Documents/Revature/Group Project/Yelp/nlp/english_only_reviews.json', \n",
    "    samplesize = 100000\n",
    ")\n",
    "\n",
    "print(f'This model represents {round(score*100,2)}% of the sentiment in the text.')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "This is marginally better, but not statistically so. Still, its moving in the right direction. And given the likely increase in sample size soon, it seems a viable option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearSVR(\n",
    "    epsilon=0.0,\n",
    "    tol = 1e-4,\n",
    "    C=1.0,\n",
    "    loss='squared_epsilon_insensitive',\n",
    "    fit_intercept=True,\n",
    "    intercept_scaling=1.0,\n",
    "    dual=False,\n",
    "    verbose=1,\n",
    "    random_state=None,\n",
    "    max_iter=1000\n",
    ")\n",
    "\n",
    "svr_dual_false_E2i_1000000, score = training(\n",
    "    model = model, \n",
    "    filein = '/home/srwight/Documents/Revature/Group Project/Yelp/nlp/english_only_reviews.json', \n",
    "    samplesize = 1000000\n",
    ")\n",
    "\n",
    "print(f'This model represents {round(score*100,2)}% of the sentiment in the text.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}